{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21230d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- 6D(前两“行”) <-> R（行正交） --------\n",
    "def r6d_rows_to_mat(r6: np.ndarray) -> np.ndarray:\n",
    "    r1 = r6[..., 0:3]\n",
    "    r2 = r6[..., 3:6]\n",
    "    e1 = r1 / np.clip(np.linalg.norm(r1, axis=-1, keepdims=True), 1e-9, None)\n",
    "    proj = np.sum(r2 * e1, axis=-1, keepdims=True) * e1\n",
    "    u2 = r2 - proj\n",
    "    e2 = u2 / np.clip(np.linalg.norm(u2, axis=-1, keepdims=True), 1e-9, None)\n",
    "    e3 = np.cross(e1, e2, axis=-1)\n",
    "    return np.stack([e1, e2, e3], axis=-2)\n",
    "\n",
    "def mat_to_r6d_rows(R: np.ndarray) -> np.ndarray:\n",
    "    return np.concatenate([R[..., 0, :], R[..., 1, :]], axis=-1)\n",
    "\n",
    "# -------- 读取列并裁前 29 维 --------\n",
    "def _stack_column_to_ndarray(col):\n",
    "    arr = col.to_numpy(zero_copy_only=False)\n",
    "    arr = [x for x in arr if x is not None]\n",
    "    if not arr:\n",
    "        return None\n",
    "    first = arr[0]\n",
    "    if isinstance(first, (list, np.ndarray)):\n",
    "        arr = np.stack([np.asarray(x, dtype=np.float64) for x in arr], axis=0)\n",
    "    else:\n",
    "        arr = np.asarray(arr, dtype=np.float64)[:, None]\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Expect 2D array, got shape {arr.shape}\")\n",
    "    return arr\n",
    "\n",
    "def read_state_29(path):\n",
    "    table = pq.read_table(path, columns=[COLUMN])\n",
    "    arr = _stack_column_to_ndarray(table[COLUMN])\n",
    "    if arr is None or arr.size == 0:\n",
    "        raise ValueError(\"empty column\")\n",
    "    if arr.shape[1] < 29:\n",
    "        raise ValueError(f\"dim={arr.shape[1]} < 29\")\n",
    "    arr = arr[:, :29]  # 固定取前29维\n",
    "    mask = np.isfinite(arr).all(axis=1)\n",
    "    return arr[mask]\n",
    "\n",
    "# -------- action 构造（相对 xyz 与相对旋转 + 夹爪绝对）--------\n",
    "def actions_from_sequence(seq29: np.ndarray, horizon: int = 40) -> np.ndarray:\n",
    "    T, D = seq29.shape\n",
    "    assert D == 29\n",
    "    W = max(T - horizon, 0)\n",
    "    if W == 0:\n",
    "        return np.empty((0, D), dtype=np.float64)\n",
    "\n",
    "    def split_blocks(x):\n",
    "        torso_xyz = x[..., 0:3];  torso_r6 = x[..., 3:9]\n",
    "        l_xyz = x[..., 9:12];     l_r6 = x[..., 12:18]; l_g = x[..., 18:19]\n",
    "        r_xyz = x[..., 19:22];    r_r6 = x[..., 22:28]; r_g = x[..., 28:29]\n",
    "        return torso_xyz, torso_r6, l_xyz, l_r6, l_g, r_xyz, r_r6, r_g\n",
    "\n",
    "    base_idx = np.arange(W, dtype=np.int64)\n",
    "    k_idx = np.arange(1, horizon + 1, dtype=np.int64)\n",
    "    future_idx = base_idx[:, None] + k_idx[None, :]\n",
    "    now_idx = base_idx[:, None]\n",
    "\n",
    "    now = seq29[now_idx]      # [W,1,29]\n",
    "    fut = seq29[future_idx]   # [W,H,29]\n",
    "\n",
    "    (now_tx, now_tr6, now_lx, now_lr6, now_lg, now_rx, now_rr6, now_rg) = split_blocks(now)\n",
    "    (fut_tx, fut_tr6, fut_lx, fut_lr6, fut_lg, fut_rx, fut_rr6, fut_rg) = split_blocks(fut)\n",
    "\n",
    "    # xyz 差分\n",
    "    d_tx = fut_tx - now_tx\n",
    "    d_lx = fut_lx - now_lx\n",
    "    d_rx = fut_rx - now_rx\n",
    "\n",
    "    # 旋转差\n",
    "    R_now_t = r6d_rows_to_mat(np.squeeze(now_tr6, 1))\n",
    "    R_now_l = r6d_rows_to_mat(np.squeeze(now_lr6, 1))\n",
    "    R_now_r = r6d_rows_to_mat(np.squeeze(now_rr6, 1))\n",
    "\n",
    "    W, H = fut_tr6.shape[0], fut_tr6.shape[1]\n",
    "    R_fut_t = r6d_rows_to_mat(fut_tr6.reshape(-1, 6)).reshape(W, H, 3, 3)\n",
    "    R_fut_l = r6d_rows_to_mat(fut_lr6.reshape(-1, 6)).reshape(W, H, 3, 3)\n",
    "    R_fut_r = r6d_rows_to_mat(fut_rr6.reshape(-1, 6)).reshape(W, H, 3, 3)\n",
    "\n",
    "    Rt_now_t = np.transpose(R_now_t, (0, 2, 1))[:, None, :, :]\n",
    "    Rt_now_l = np.transpose(R_now_l, (0, 2, 1))[:, None, :, :]\n",
    "    Rt_now_r = np.transpose(R_now_r, (0, 2, 1))[:, None, :, :]\n",
    "\n",
    "    Rrel_t = R_fut_t @ Rt_now_t\n",
    "    Rrel_l = R_fut_l @ Rt_now_l\n",
    "    Rrel_r = R_fut_r @ Rt_now_r\n",
    "\n",
    "    d_tr6 = mat_to_r6d_rows(Rrel_t)\n",
    "    d_lr6 = mat_to_r6d_rows(Rrel_l)\n",
    "    d_rr6 = mat_to_r6d_rows(Rrel_r)\n",
    "\n",
    "    # 夹爪：未来绝对\n",
    "    a_lg = fut_lg\n",
    "    a_rg = fut_rg\n",
    "\n",
    "    A = np.concatenate([d_tx, d_tr6, d_lx, d_lr6, a_lg, d_rx, d_rr6, a_rg], axis=-1)  # [W,H,29]\n",
    "    return A.reshape(-1, 29)\n",
    "\n",
    "# -------- 合并 mean/std/min/max --------\n",
    "def merge_basic(stats_list):\n",
    "    Ns = np.array([s[\"N\"] for s in stats_list], dtype=np.int64)\n",
    "    means = np.stack([s[\"mean\"] for s in stats_list], 0)\n",
    "    stds  = np.stack([s[\"std\"]  for s in stats_list], 0)\n",
    "    mins  = np.stack([s[\"min\"]  for s in stats_list], 0)\n",
    "    maxs  = np.stack([s[\"max\"]  for s in stats_list], 0)\n",
    "\n",
    "    N_total = int(Ns.sum())\n",
    "    w = (Ns / max(N_total, 1))[:, None]\n",
    "    mean_g = (w * means).sum(0)\n",
    "\n",
    "    var_within  = ((Ns - 1)[:, None] * (stds ** 2)).sum(0)\n",
    "    var_between = (Ns[:, None] * (means - mean_g[None, :])**2).sum(0)\n",
    "    denom = max(N_total - 1, 1)\n",
    "    var_g = (var_within + var_between) / denom\n",
    "    std_g = np.sqrt(np.maximum(var_g, 0.0))\n",
    "\n",
    "    return {\n",
    "        \"N_total\": N_total,\n",
    "        \"mean\": mean_g,\n",
    "        \"std\":  std_g,\n",
    "        \"min\":  mins.min(0),\n",
    "        \"max\":  maxs.max(0),\n",
    "    }\n",
    "\n",
    "def quantiles_from_hist_1d(counts, edges, q):\n",
    "    cdf = np.cumsum(counts)\n",
    "    total = max(int(cdf[-1]), 1)\n",
    "    target = q * total\n",
    "    idx = np.argmax(cdf >= target)\n",
    "    left_cum = cdf[idx-1] if idx > 0 else 0\n",
    "    right_cum = cdf[idx]\n",
    "    left_edge, right_edge = edges[idx], edges[idx+1]\n",
    "    bin_count = max(right_cum - left_cum, 1)\n",
    "    frac = (target - left_cum) / bin_count\n",
    "    return left_edge + (right_edge - left_edge) * frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8678e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- 主流程：单文件输出（state + action） ----------------\n",
    "def main():\n",
    "    files = []\n",
    "    for root, _, fs in os.walk(DATA_DIR):\n",
    "        for fname in fs:\n",
    "            if fname.endswith(\".parquet\"):\n",
    "                files.append(os.path.join(root, fname))\n",
    "    print(f\"Found {len(files)} parquet files\")\n",
    "    if not files:\n",
    "        return\n",
    "\n",
    "    # Pass1：逐文件统计 state/action 的 mean/std/min/max，并汇总全局 min/max\n",
    "    state_stats, action_stats = [], []\n",
    "    state_min_g, state_max_g = None, None\n",
    "    action_min_g, action_max_g = None, None\n",
    "\n",
    "    for f in tqdm(files, desc=\"Pass1: basic stats (state & action)\"):\n",
    "        try:\n",
    "            S = read_state_29(f)  # [T,29]\n",
    "            if S.size == 0: \n",
    "                continue\n",
    "\n",
    "            # state per-file\n",
    "            Ns = S.shape[0]\n",
    "            s_mean = S.mean(0)\n",
    "            s_std  = S.std(0, ddof=1) if Ns > 1 else np.zeros(29)\n",
    "            s_min  = S.min(0); s_max = S.max(0)\n",
    "            state_stats.append({\"N\": Ns, \"mean\": s_mean, \"std\": s_std, \"min\": s_min, \"max\": s_max})\n",
    "            state_min_g = s_min if state_min_g is None else np.minimum(state_min_g, s_min)\n",
    "            state_max_g = s_max if state_max_g is None else np.maximum(state_max_g, s_max)\n",
    "\n",
    "            # action per-file\n",
    "            if len(S) > FUTURE_HORIZON:\n",
    "                A = actions_from_sequence(S, FUTURE_HORIZON)  # [N,29]\n",
    "                if A.size > 0:\n",
    "                    Na = A.shape[0]\n",
    "                    a_mean = A.mean(0)\n",
    "                    a_std  = A.std(0, ddof=1) if Na > 1 else np.zeros(29)\n",
    "                    a_min  = A.min(0); a_max = A.max(0)\n",
    "                    action_stats.append({\"N\": Na, \"mean\": a_mean, \"std\": a_std, \"min\": a_min, \"max\": a_max})\n",
    "                    action_min_g = a_min if action_min_g is None else np.minimum(action_min_g, a_min)\n",
    "                    action_max_g = a_max if action_max_g is None else np.maximum(action_max_g, a_max)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {f}: {e}\")\n",
    "\n",
    "    if not state_stats:\n",
    "        print(\"No state stats computed.\"); return\n",
    "    if not action_stats:\n",
    "        print(\"No action stats computed.\"); return\n",
    "\n",
    "    merged_state  = merge_basic(state_stats)\n",
    "    merged_action = merge_basic(action_stats)\n",
    "\n",
    "    # Pass2：直方图法 q01/q99（state & action）\n",
    "    def run_hist_quantiles(files, is_action: bool, lo, hi):\n",
    "        D = 29\n",
    "        all_counts = np.zeros((D, HIST_BINS), dtype=np.int64)\n",
    "        edges = [np.linspace(lo[d]-1e-9, hi[d]+1e-9, HIST_BINS+1) for d in range(D)]\n",
    "        desc = \"Pass2: hist (action)\" if is_action else \"Pass2: hist (state)\"\n",
    "        for f in tqdm(files, desc=desc):\n",
    "            try:\n",
    "                S = read_state_29(f)\n",
    "                if S.size == 0: continue\n",
    "                X = actions_from_sequence(S, FUTURE_HORIZON) if is_action else S\n",
    "                if X.size == 0: continue\n",
    "                for d in range(D):\n",
    "                    c, _ = np.histogram(X[:, d], bins=edges[d])\n",
    "                    all_counts[d] += c\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] {f}: {e}\")\n",
    "        q01 = np.empty(D); q99 = np.empty(D)\n",
    "        for d in range(D):\n",
    "            q01[d] = quantiles_from_hist_1d(all_counts[d], edges[d], 0.01)\n",
    "            q99[d] = quantiles_from_hist_1d(all_counts[d], edges[d], 0.99)\n",
    "        return q01, q99\n",
    "\n",
    "    if USE_HIST_FOR_QUANTILES:\n",
    "        q01_s, q99_s = run_hist_quantiles(files, False, merged_state[\"min\"], merged_state[\"max\"])\n",
    "        q01_a, q99_a = run_hist_quantiles(files, True,  merged_action[\"min\"], merged_action[\"max\"])\n",
    "    else:\n",
    "        raise NotImplementedError(\"Enable USE_HIST_FOR_QUANTILES for accurate q01/q99.\")\n",
    "\n",
    "    # 单文件输出\n",
    "    os.makedirs(os.path.dirname(OUT_JSON), exist_ok=True)\n",
    "    payload = {\n",
    "        \"norm_stats\": {\n",
    "            \"state\": {\n",
    "                \"mean\": merged_state[\"mean\"].tolist(),\n",
    "                \"std\":  merged_state[\"std\"].tolist(),\n",
    "                \"min\":  merged_state[\"min\"].tolist(),\n",
    "                \"max\":  merged_state[\"max\"].tolist(),\n",
    "                \"q01\":  q01_s.tolist(),\n",
    "                \"q99\":  q99_s.tolist(),\n",
    "            },\n",
    "            \"action\": {\n",
    "                \"mean\": merged_action[\"mean\"].tolist(),\n",
    "                \"std\":  merged_action[\"std\"].tolist(),\n",
    "                \"min\":  merged_action[\"min\"].tolist(),\n",
    "                \"max\":  merged_action[\"max\"].tolist(),\n",
    "                \"q01\":  q01_a.tolist(),\n",
    "                \"q99\":  q99_a.tolist(),\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    with open(OUT_JSON, \"w\") as f:\n",
    "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"[DONE] Both stats -> {OUT_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73299a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3151 parquet files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass1: basic stats (state & action): 100%|██████████| 3151/3151 [02:11<00:00, 24.03it/s] \n",
      "Pass2: hist (state): 100%|██████████| 3151/3151 [00:09<00:00, 316.84it/s]\n",
      "Pass2: hist (action): 100%|██████████| 3151/3151 [02:21<00:00, 22.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Both stats -> /kpfs-cognition/waikei/codes/openpi-uncle/assets/diversity_partial/norm_stats_both.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#================= 配置 =================\n",
    "DATA_DIR = \"/kpfs-regular/share_space/data/lerobot_data_aliyun/s1_data/diversity_partial\"\n",
    "COLUMN   = \"cartesian_so3_dict.cartesian_pose_state\"\n",
    "# COLUMN = ['cartesian_so3_dict.cartesian_pose_state', 'cartesian_so3_dict.cartesian_pose_command']\n",
    "\n",
    "OUT_JSON = \"/kpfs-cognition/waikei/codes/openpi-uncle/assets/diversity_partial/norm_stats_both.json\"\n",
    "\n",
    "FUTURE_HORIZON = 40\n",
    "USE_HIST_FOR_QUANTILES = True\n",
    "HIST_BINS = 2048\n",
    "#=======================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317e0be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
